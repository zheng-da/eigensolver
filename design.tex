\section{Design}
FlashEigen is an external-memory eigensolver optimized for any fast I/O devices
such as a large SSD array to compute eigenvalues of sparse graphs. It takes
advantage of the flexible programming interface of the Anasazi framework and
focuses on optimizing sparse matrix dense matrix multiplication and dense
matrix operations on SSDs.

\subsection{Eigensolver algorithm}

%\begin{figure}
%\centering
%\includegraphics[scale=0.35]{./SpMM.pdf}
%\vspace{-5pt}
%\caption{}
%\vspace{-5pt}
%\label{SpMM}
%\end{figure}

\begin{algorithm}
	\begin{algorithmic}[1]
		\For{i = 0, 1, ..., until convergence}
		\State (1) Update the subspace $S \in \mathbb{R}^{n \times m}$,
		\State (2) Solve the projected eigenproblem $S^TASy = S^TSy\theta$.
		\State (3) Compute the residual: $r = Kx - x\theta$, where
		\State\hspace{\algorithmicindent} $x = Sy$ (Ritz vector), $\theta = \rho(x)$ (Ritz value).
		\State (4) Test the convergence of a Ritz pair $(x, \rho(x))$.
		\EndFor
	\end{algorithmic}
	\caption{Pseudo code of a generic eigenvalue algorithm that compute eigenvalues
	of a square matrix $A$ with $n$ rows and columns.}
	\label{eigencode}
\end{algorithm}

The state-of-art eigenvalue algorithms compute eigenvalues with iterative
methods. Figure \ref{eigencode} shows the general steps used by the eigenvalue
algorithms.
Step (1) constructs a vector subspace $S \in \mathbb{R}^{n \times m}$, where
$n$ is the number of rows and columns of a sparse matrix and $m$ is the number
of vectors in the subspace. When computing eigenvalues of a sparse graph,
two key operations in this step are sparse matrix multiplication to construct
the subspace and reorthogonalization to correct float-point rounding errors.
A block extenion of an eigensolver updates multiple vectors in the subspace
in a single step, which leads to sparse matrix dense matrix multiplication.
Step (2) projects the large sparse matrix to a much smaller matrix with only
$m$ rows and $m$ columns, which can be solved by other eigensolvers such as
the one in LAPACK \cite{lapack}. Step (3) projects the solution of the small
eigenvalue problem back to the original eigenvalue problem. Step (4) tests
whether the projected solution fulfills the precision requirement given by
users. If not, the algorithm may adjust the vector subspace, jump back to
step (1) and continue the process.

\subsection{The architecture of FlashEigen}

We build FlashEigen on top of SAFS, a user-space filesystem, to fully utilize
the I/O throughput of a large SSD array. The architecture of FlashEigen is shown
in Figure \ref{arch}. FlashEigen has two main components that process sparse
matrices and dense matrices respectively. It stores data of sparse matrices
and dense matrices
in SAFS and implements matrix operations required by the Anasazi eigensolvers.
As such, the Anasazi eigensolvers can access data from SSDs.

\begin{figure}
\centering
\includegraphics[scale=0.4]{./architecture.pdf}
\vspace{-5pt}
\caption{The architecture of FlashEigen.}
\vspace{-5pt}
\label{arch}
\end{figure}

\subsection{SAFS}

SAFS \cite{safs} is a user-space filesystem for a high-speed SSD array in
a NUMA machine. It is implemented as a library and runs in the address space
of its application. It is deployed on top of the Linux native filesystem.
SAFS was originally designed for optimizing small I/O accesses. However,
sparse matrix multiplication and dense matrix operations
generate much fewer but much larger I/O. Therefore, we provide additional
optimizations to maximize sequential I/O throughput from a large SSD array.

The original SAFS has a dedicated I/O thread for each SSD. The I/O thread
accesses the SSD exclusively to avoid lock contention in the Linux kernel.
Application threads have to send I/O requests to one of the I/O threads
with message passing when accessing data from SSDs. It is necessary to have
one I/O thread for
an SSD when applications issue many small I/O accesses because processing
a large number of I/O accesses consumes a significant number of CPU cycles.
However, when the workload only has large I/O requests, each I/O request takes
much longer time to complete. As a result, the I/O threads are constantly put
into sleep while waiting for I/O and each I/O completion may suffer from
the latency of a thread context switch.

The latency of a thread context switch becomes noticeable on a high-speed SSD
array under a sequential I/O workload and it becomes critical to avoid thread
context switch to gain I/O performance. Therefore, instead of having an I/O
thread for each SSD, we use only a single I/O thread for each NUMA node, which
is responsible for all of the SSDs connected to the NUMA node. As such, an I/O
thread processes many more I/O requests to amortize the latency of a context
switch. Similarly, application threads communicate with I/O threads through
message passing when issuing I/O requests. If the computation in application
threads did not saturate CPU, SAFS would put the application threads into
sleep while they were waiting for I/O. This results in many thread context
switches and underutilization of both CPU and SSDs. To saturate I/O,
an application thread issues asynchronous I/O and poll for I/O to avoid thread
context switches after completing all computation available to it.

To better support access to many relatively small files simultaneously, SAFS
stripes data in a file across SSDs with a different striping order for each file.
This strategy stores data from multiple files evenly across SSDs and improves
I/O utialization. Due to the sequential I/O workload, FlashEigen stripes data
across SSDs with a large block size, in the order of multiple
megabytes, to increase I/O throughput and potentially reduce write amplification
on SSDs \cite{}. Such a large block size may cause storage skew for small files
on a large SSD array if every file stripes data in the same order. Using
the same striping order for all files may also cause skew in I/O access.
Therefore, SAFS generates a random striping order for each file to evenly
distribute I/O among SSDs when a file is created. SAFS stores the striping
order with the file for future data retrieval.

\subsection{Sparse matrix multiplication} \label{spmm}
Sparse matrix multiplication is a computationally expensive
operation in an eigensolver due to random memory access. Sparse matrix vector
multiplication (SpMV) is usually limited by the random memory performance of
DRAM. Sparse matrix dense matrix multiplication (SpMM) increases data locality
and improve overall performance of an eigensolver. Therefore, a block extension
of an eigensolver is preferred.

To scale sparse matrix multiplication to a sparse graph with billions of vertices,
we perform sparse matrix multiplication in semi-external memory (SEM). That is,
we keep the vectors or dense matrices in memory and the sparse
matrix on SSDs. This strategy enables nearly in-memory performance while achieving
the scalability in proportion to the ratio of edges to vertices in a graph.

\subsubsection{The sparse matrix format}
The state-of-art numeric libraries store a sparse matrix in compressed row storage
(CSR) or compressed column storage (CSC) format. However, these formats incur
many CPU cache misses in sparse matrix multiplication on many real-world graphs
due to their nearly random vertex connection. They also require a relatively
large storage size. For a graph with billions of edges, we have to use eight
bytes to store the row and column indices. For semi-external memory sparse
matrix multiplication, SSDs may become the bottleneck if a sparse matrix has
a large storage size.
Therefore, we need to use an alternative format for sparse matrices to increase
CPU cache hits and reduce the amount of data read from SSDs.

\begin{figure}
\centering
\includegraphics[scale=0.3]{./sparse_mat.pdf}
\vspace{-5pt}
\caption{}
\vspace{-5pt}
\label{sparse_mat}
\end{figure}

To increase CPU cache hits, we deploy cache blocking \cite{Im04} and store
non-zero entries of a sparse matrix in tiles (Figure \ref{sparse_mat}).
When a tile is small, the rows from the input and output dense matrices
involved in the multiplication with the tile are always kept in the CPU cache
during the multiplication. The optimal tile size should fill the CPU cache
with the rows of the dense matrices involved in the multiplication with
the tile and is affected by the number of columns of the dense matrices,
which are chosen by users. Instead of generating a sparse matrix with
different tile sizes optimized for different numbers of columns in the dense
matrices, we use a relatively small tile size and rely on the runtime system
to optimize for different numbers of columns (in section \ref{sec:exec}).
In the semi-external memory, we expect that the dense matrices do not
have more than eight columns in sparse matrix multiplication. Therefore, we
use the tile size of $16K \times 16K$ by default to balance the matrix storage
size and the adaptibility to different numbers of columns.

\begin{figure}
\centering
\includegraphics[scale=0.5]{./tile_format.pdf}
\vspace{-5pt}
\caption{The storage format of a tile in a sparse matrix.}
\vspace{-5pt}
\label{tile_format}
\end{figure}

To reduce the overall storage size of a sparse matrix, we use a compact format
to store non-zero entries in a tile. In very sparse matrices such as
many real-world graphs, many rows in a tile do not have any non-zero entries.
The CSR (CSC) format requires an entry for each row (column) in the row
(column) index. Therefore, the CSR or CSC format waste space when storing elements
in a tile. Instead, we only keep data for rows with non-zero entries in a tile
shown in Figure \ref{tile_format} and refer to this format as SCSR (Super
Compressed Row Storage). This format maintains a row header for each non-empty
row. A row header has an identifier to indicate the row number, followed by
column indices. 
The most significant bit of the identifier is always set to 1, while the most
significant bit of a column index entry is always set to 0. As such, we can easily
distinguish a row identifier from a column index entry and determine the end
of a row. Thanks to the small size of a tile, we use two bytes to further store a row
number and a column index entry to reduce the storage size. Since the most
significant bit is used to indicate the beginning of a row, this format allows
a maximum tile size of $32K \times 32K$.

For many real-world graphs, many rows in a tile have only one non-zero entries,
thanks to their sparsity and nearly random vertex connection. Storing these
single-entry rows in the SCSR format results in many conditional jump CPU
instructions in sparse matrix multiplication.
In contrast, the coordinate format (COO) is more suitable for storing these
single-entry rows. It does not increase the storage size but significantly
reduces the number of conditional jump instructions when we iterate
them. As a result, we hybrid SCSR and COO to store non-zero entries in a tile
with COO stored behind the row headers of SCSR. All non-zero entries are
stored together at the end of a tile.

We organize tiles in a sparse matrix in tile rows and maintain a matrix index
for them. Each entry of the index stores the location of a tile row on SSDs
to facilitate random access
to tile rows. This is useful for parallelizing sparse matrix multiplication.
Because a tile contains thousands of rows, the matrix index requires a very
small storage size even for a billion-node graph. We keep the entire index
in memory during sparse matrix multiplication.

\subsubsection{The dense matrix format for SpMM} \label{numa_mat}
Dense matrices in sparse matrix multiplication are tall-and-skinny matrices
with millions or even billions of rows but only several columns. We organize
elements of the dense matrices in row-major order to increase data locallity,
as shown in Figure \ref{dense_mat} (a).

For a non-uniform memory architecture (NUMA), we partition the input dense matrix
horizontally and store partitions evenly across NUMA nodes to fully utilize
the bandwidth of memory and inter-processor links in sparse matrix
multiplication. The NUMA architecture is commonly used in today's multi-processor
servers, where each processor connects to its own memory banks. It is essential
to fully utilize the bandwidth of memory and inter-processor links to achieve
performance. As shown in Figure \ref{dense_mat} (a), we assign multiple
contiguous rows in a row interval to a partition, which is assigned to a NUMA
node. A row interval always has $2^i$ rows for efficiently locating a row
with bit operations. The row interval size is also multiple of the tile size of
a sparse matrix so that multiplication on a tile only needs to access rows
from a single row interval.

\begin{figure}
\centering
\includegraphics[scale=0.4]{./dense_matrix.pdf}
\vspace{-5pt}
\caption{The data layout of tall-and-skinny dense matrices. A tall-and-skinny
dense matrix is partitioned horizontally into many row intervals.
(a) For an in-memory matrix, row intervals are stored across NUMA nodes and
elements are stored in row-major order; (b) for an SSD-based matrix, elements
inside a row interval are stored in column-major order.}
\vspace{-5pt}
\label{dense_mat}
\end{figure}

\subsubsection{Execution of sparse matrix multiplication} \label{sec:exec}
We perform sparse matrix dense matrix multiplication in semi-external memory
and optimize it for different numbers of columns in the dense matrices.

We partition a sparse matrix horizontally for parallelization and assign multiple
contiguous tile rows to the same partition (Figure \ref{sparse_mat}).
The number of tile rows assigned to a partition is determined at runtime
based on the number of columns in the input dense matrix. A thread reads
a partition of the sparse matrix asynchronously from SSDs. Once a partition
is ready in memory, the worker thread multiplies the partition with the input
dense matrix. The thread processes one tile at a time and stores
the intermediate result in a buffer allocated in the local memory to reduce
remote memory access. To reduce memory consumption,
we write the portion of the output dense matrix to SSDs immediately whenever
it is generated. \dz{Implement this.}

To better utilize CPU cache, we process tiles of a partition in
\textit{super tile}s (Figure \ref{sparse_mat}). The tile size of a sparse
matrix is specified when the sparse matrix image is created and is relatively
small to handle different numbers of columns in the dense matrices.
A \textit{super tile} is composed of tiles from multiple tile rows and its
size is determined at runtime by three factors: the number of columns
in the dense matrices, the CPU cache size and the number of threads that
share the CPU cache. An optimal size for a \textit{super tile} fills
the CPU cache with the rows from the dense matrices involved in
the computation with the \textit{super tile}.

Load balancing also plays a key role in sparse matrix multiplication on
many real-world graphs due to their power-law distribution in vertex degree.
In FlashEigen, a worker thread first processes partitions originally assigned
to the thread. When a worker thread finishes
all of its own partitions, it steals partitions that have not been processed
from other worker threads.

In spite of nearly random edge connection in a real-world graph,
there exists regularity that allows vectorization to improve performance
in sparse matrix dense matrix multiplication. For each non-zero entry, we
need to multiply it with the corresponding row from the input dense matrix
and add the result to the corresponding row in the output dense matrix.
These operations can be accomplished by the vector CPU instructions such as
AVX \cite{avx}. The current implementation relies on GCC's auto-vectorization
to translate the C code to the vector CPU instructions by predefining the matrix
width in the code.

\subsection{Dense matrix operations}
The vector subspace required by an eigensolver is massive when the eigensolver
computes eigenvalues of a billion-node graph or computes many eigenvalues
of a multi-million-node graph. The number of vectors in the subspace
increases with the number of required eigenvalues. Furthermore, a larger
number of vectors in the subspace improves the convergence rate of an eigensolver. 
It is often that the storage size required by the subspace is larger than
the sparse matrix for eigendecomposition on many real-world graphs. Therefore,
FlashEigen stores these vectors on SSDs.

The Anasazi framework provides a set of programming interface to expose
the vectors in the subspace to users as dense matrices and allow users to
redefine the dense matrices and the operations on them. FlashEigen implements
the programming interface and stores the vectors of the subspace on SSDs.
The Anasazi eigensolvers update multiple vectors of the subspace in an iteration
due to the block extension, so FlashEigen stores multiple vectors in a dense
matrix physically and the number of vectors in a matrix is determined by
the block size of a block eigensolver. This design also helps garbage
collection in lazy evaluation (section \ref{sec:lazy_eval}).
As such, the subspace is composed of multiple tall-and-skinny dense matrices.

\begin{table}
	\begin{center}
		\small
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			& operation & customized output \\
			\hline
			op1 & $CC \leftarrow \alpha \times AA \times B + \beta \times CC$ & yes \\
			\hline
			op2 & $BB \leftarrow AA \times diag(vec)$ & yes \\
			\hline
			op3 & $A \leftarrow \alpha \times t(AA) \times BB$ & no \\
			\hline
			op4 & $CC \leftarrow \alpha \times AA + \beta \times BB$ & yes \\
			\hline
			op5 & $BB \leftarrow \alpha \times AA$ & yes \\
			\hline
			op6 & $vec \leftarrow norm\_col(AA)$ & no \\
			\hline
			op7 & $BB \leftarrow AA[,idxs]$ & yes \\
			\hline
			op8 & $AA[,idxs] \leftarrow BB$ & yes \\
			\hline
			op9 & $AA \leftarrow conv\_layout(BB)$ & yes \\
			\hline
		\end{tabular}
		\normalsize
	\end{center}
	\caption{The dense matrix operations required by the Anasazi eigensolvers.
		$XX$ represents a tall dense matrix, $X$ represents a small dense matrix,
	$\alpha$ and $\beta$ represents scalar variables.}
	\label{anasazi_ops}
\end{table}

FlashEigen requires a set of dense matrix operations shown in Table
\ref{anasazi_ops}. $op1-8$ are the operations required by the Anasazi
framework. The most computationally expensive operations are the two
matrix multiplication operations: $op1$ and $op3$, mainly used for
reorthogonalization to fix float-point rounding errors. The eigensolvers
use $op7$ and $op8$ to access individual columns of a dense matrix,
so we store the dense matrices in column major by default.
However, the sparse matrix dense matrix multiplication described in section
\ref{spmm} requires a row-major dense matrix to increase data locality.
Thus, FlashEigen adds another operation $op9$ to convert data layout
in dense matrices, which converts a column-major matrix to a row-major
matrix when it is passed to the SpMM operation.

It is challenging to achieve the performance of external-memory dense matrix
operations comparable to their in-memory counterparts. Unlike sparse matrix
multiplication, these dense matrix operations are less memory intensive.
Even though SSDs are fast, their sequential I/O performance is still an order
of magnitude slower than RAM.
Furthermore, SSDs wears out after a certain amount of write \cite{}.
Even enterprise SSDs \cite{} only allows a small number of DWPD
(diskful writes per day). Therefore, FlashEigen optimizes dense matrix operations
with three goals: \textit{(i)} maximizing I/O throughput from SSDs,
\textit{(ii)} minimizing the amount of data read from SSDs,
\textit{(iii)} reducing SSD wearout.

%\begin{figure}
%	\begin{center}
%		\footnotesize
%		\vspace{-15pt}
%		\include{mat_ops}
%		\vspace{-15pt}
%		\caption{The relative performance of external-memory matrix operations
%			on a dense matrix with 200M rows and 16 columns on an array of 24
%		SSDs.}
%		\label{perf:mat_ops}
%	\end{center}
%\end{figure}

\subsubsection{Parallelization and external memory access} \label{par_em}
We partition the tall-and-skinny matrices horizontally for parallelization
and external-memory access. Figure \ref{dense_mat} (b) illustrates the format
of an external-memory tall-and-skinny matrix. Like a NUMA dense matrix in
Figure \ref{dense_mat} (a),
an external-memory matrix is divided into multiple row intervals and data
in a row interval is stored contiguously. Unlike a NUMA dense matrix, elements
in a row interval of an external-memory matrix are stored in column-major order
for easily accessing individual columns. The size of a row interval is chosen
according to the number of columns in the matrix to generate large I/O requests,
in the order of megabytes.

Once data in a row interval is loaded to memory, we further partition it to
smaller row intervals so that data
in the sub-row interval fits in CPU cache. \dz{TODO: The size of the sub row
interval should also adapt to the matrix width.} This optimization significantly
increase CPU cache hits in a sequence of dense matrix operations constructed in
the lazy evaluation (Section \ref{sec:lazy_eval}).

To parallelize a matrix operation, we assign one row interval at a time to
a worker thread. When a worker thread gets a row interval, it owns the row
interval and is responsible for accessing the data in the row interval from
all of the tall-and-skinny matrices and performing computation on them.
When processing a row interval, a worker thread does not need to access data
from another row interval on SSDs.
%(Figure \ref{fig:mat_par}).
Majority of the operations in Table \ref{anasazi_ops} output tall-and-skinny
matricies whose rows only depend on the same rows from the input matrices.
For these operations, the computation and data access to the tall-and-skinny
matrices are completely independant between row intervals. One of the exceptions is
$op3$, which outputs a small matrix. However, this operation can be split into
two sub-operations: the first one performs computation on data in the same row
interval from all of the input matrices and output a small matrix and the computation
is completely independant between row intervals; the second one sums all of
the small matrices and outputs a single small matrix. The first sub-operation
has most of computation in $op3$ and requires external-memory data access,
so we parallelize it in the same fashion as other operations.
%This is illustrated in Figure \ref{fig:mat_par}.
$op6$ can be evaluated in a similar fashion to $op3$. Therefore, all matrix
operations in Table \ref{anasazi_ops} can be parallelized in the same fashion.

%\dz{a large write is required to achieve persistent write performance and
%wearout?}
%We maintain a global work queue of row intervals and dispatch one row interval
%to a thread at a time. This strategy allows worker threads to process row
%intervals close to each other, which helps to accumulate very large writes to SSDs.

\begin{figure}
\centering
\includegraphics[scale=0.4]{./mat_group.pdf}
\vspace{-5pt}
\caption{Break a large group of dense matrices in an operation into multiple
small groups to reduce memory consumption. XX indicates a tall-and-skinny matrix
stored on SSDs and X indicates a small matrix stored in memory.}
\vspace{-5pt}
\label{fig:mat_group}
\end{figure}

The Anasazi eigensolvers frequently perform a matrix operation on many
tall-and-skinny matrices and the number of the matrices varies in each iteration.
The number of matrices can be as large as multiple hundred when an eigensolver
computes hundreds of eigenvalues. $op1$ and $op3$ are such operations.
If a thread has to read the data in a row interval from all of the matrices
before performing computation,
the amount of data in memory is still very large. Another option is to read
only part of a partition, which results in many small reads and writes to SSDs,
because the matrices are organized in column-major order.

Instead, we break a large group of tall-and-skinny matrices into multiple small
groups to reduce
memory consumption when evaluating these operations on them. Figure
\ref{fig:mat_group} illustrates this optimization on $op1$ and $op3$. For $op1$,
we split the small dense matrix horizontally and each group of tall-and-skinny
matrices gets a partition of the small dense matrix. Each group generates
a intermediate tall-and-skinny matrix conceptually and we apply an addition
operation on all intermediate matrices to generate the final result.
Materializing these
intermediate matrices would result in large memory consumption if we store them
in memory or a large amount of I/O if we store them on SSDs. Instead, we leverage
the lazy evaluation in Section \ref{sec:lazy_eval} and only materialize part of
the intermediate matrices at a time and passes the materialized parts to
the addition operation to generate the final result.

For $op3$, an eigensolver usually transopses a group of tall-and-skinny matrices
and multiply them with a tall-and-skinny matrix. We apply a similar strategy to
$op3$. We break the large group of dense matrices into multiple groups and each
group shares the same tall-and-skinny matrix in the right operand. Each group
generates
a small matrix that can be kept in memory. In this case, all input matrices
are large and are stored on SSDs. To minimize I/O, we perform operations on
each group together to share I/O for accessing the tall-and-skinny matrix
in the right operand.

%\dz{This approach actually can change the computation complexity of dense matrix
%multiplication. I need to measure its impact on in-memory dense matrix
%multiplication.}

\subsubsection{Lazy evaluation} \label{sec:lazy_eval}
Lazy evaluation avoids materializing every dense matrix to reduce the amount
of data read and written to SSDs.
To enable lazy evaluation, we define a special matrix to represent the output
of a matrix operation. Such a matrix does not store the data of
an operation result. Instead, it stores the computation and a reference to
the input matrices. We refer to these special matrices as \textit{virtual matrices}.
We can evaluate most matrix operations in Table \ref{anasazi_ops} lazily,
as long as the output matrix of an operation is a tall-and-skinny matrix. Only \textit{op3}
and \textit{op6} cannot be evaluated lazily because they output small matrices
and small vectors stored in Anasazi's native matrices and vectors.

%With \textit{virtual matrices}, we construct a directed acyclic graph (DAG)
%at runtime to represent computation in FlashEigen. In the DAG, we store
%all scalar variables and small matrices as part of computation.
%Figure \ref{comp_seq} shows an example of a sequence of dense matrix operations
%performed in FlashEigen. Figure
%\ref{dag} visualizes the sequence of computation, which forms a directed acyclic
%graph (DAG). Inside this DAG, we do not need to perform any computation other
%than the last one.

All matrices in FlashEigen are immutable so that \textit{virtual matrices}
can generate the same result every time when they are materialized. Therefore,
our implementation of the matrix operations in Table \ref{anasazi_ops} always
generates new matrices. The original Anasazi matrix operations require in-place
update on the existing dense matrices, so FlashEigen only passes a pointer to
a dense matrix to the Anasazi eigensolvers instead of the matrix data.
%This approach is equivalent to variable renaming used by compilers.
A dense matrix is garbage collected only when there are not any references to
the matrix.

%\textit{op7} and \textit{op8} access individual columns and have to be incorporated
%with the lazy evaluation. The eigensolvers usually access only one column at a time
%from a dense matrix. When accessing a single column, we materialize the virtual
%matrix in column major if the underlying matrix is a \textit{virtual matrix}.
%Because all matrices are immutable, setting a column in a matrix needs to
%copy the original matrix and set the particular column. Instead of physically
%generating the matrix, we create a virtual matrix that merge the original matrix
%with the column.

%\begin{figure}
%\begin{minted}[mathescape,
%		fontsize=\scriptsize,
%		frame=single,
%]{r}
%# MV0, MV1, MV2, MV3, MV4 are tall dense matrices.
%# B1, B2, B3, B4 are small dense matrices.
%# MV1 is the result of sparse matrix dense matrix
%# multiplication.
%MV0 <- rand_init
%MV1 <- SpMM
%MV2 <- 1 * MV0 * B1 + 0 * MV2
%MV2 <- 1 * MV2 * B2 + 0 * MV2
%MV3 <- 1 * MV1 * B3 + 0 * MV1
%MV1 <- -1 * MV2 * B4 + MV3
%MV4 <- MV1 * diag(vec)
%B <- t(MV4) * MV4
%\end{minted}
%\vspace{-5pt}
%\caption{A small sequence of dense matrix operations typically performed by
%the Anasazi eigensolvers.}
%\label{comp_seq}
%\end{figure}

%\begin{figure}
%\centering
%\includegraphics[scale=0.5]{./dag.pdf}
%\vspace{-5pt}
%\caption{A directed acyclic graph represents the sequence of matrix operations
%shown in Figure \ref{comp_seq}. Each rounded rectangular node indicates
%a virtual matrix and each rectangular node indicates a materialized matrix.}
%\vspace{-5pt}
%\label{dag}
%\end{figure}

To perform actual computation, FlashEigen needs to materialize
\textit{virtual matrices}. It materializes a \textit{virtual matrix} when
encountering \textit{op3} and \textit{op6} because these two operations
output Anasazi's native matrices and vectors.
A \textit{virtual matrix} represents some computation on some materialized
tall-and-skinny matrices stored on SSDs. FlashEigen materializes
a \textit{virtual matrix}, which has more than one materialized input matrix,
to minimize the amount of data read from SSDs.
%FlashEigen also materializes all of the dense matrices in the basis.

A \textit{virtual matrix} may contain a sequence of operations, so materializing
it may trigger matrix materialization recursively. We discard the materialized
partition of an intermediate matrix, once it is no longer needed, to avoid
writing data of an intermediate matrix to SSDs.
We partition a \textit{virtual matrix} horizontally in the same fashion as
the external-memory matrices and materialize each partation independantly. 
To increase CPU cache hits, we use a much smaller partition size than
the external-memory matrix. As such, the output of the previous operation is
still in the CPU cache when it is fed to the next operation.

\subsubsection{Matrix caching}
FlashEigen deploys two forms of matrix caching to reduce I/O to SSDs.
In the first case, FlashEigen worker threads cache part of a tall-and-skinny
matrix locally. In the other case, FlashEigen can also cache the most recent
tall-and-skinny matrix in the vector subspace.

Caching part of a tall-and-skinny matrix read from SSDs benefits some of
the matrix operations. One example is the optimization for $op3$ in section
\ref{par_em}, which breaks a large group of dense matrices into multiple
subgroups and the matrix in the right operand is shared by all of the subgroups.
A worker thread only needs to cache data in a row interval of the right matrix
that is currently being processed
because a thread processes one row interval at a time and each row interval of
the tall-and-skinny matrices is processed only once.
Therefore, a worker thread can buffers the data in a row interval of a matrix
locally and accessing the buffered data doesn't incur any locking overhead.

When we buffer the recent portions, we need to give each matrix a data identifier
to identify its data, so we can recognize which portion of data can be reused.
For certain operations, even though a new matrix is created, the data inside
remains the same. A typical example is transpose. The identifier we give to
each matrix should identify the data inside a matrix instead of individual matrices,
so a transposed matrix and its original matrix should share the same identifier.

FlashEigen also caches the most recent tall-and-skinny matrix in the vector
subspace if the RAM of a machine is sufficient to accommodate the entire matrix.
When a new matrix in the vector subspace is generated by sparse matrix
multiplication, an eigensolver needs to perform a sequence of operations on it,
which includes reorgonalization. By caching the matrix in memory, we can
significantly reduce the amount of data written to SSDs.
