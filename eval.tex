\section{Experimental Evaluation}

We evaluate the performance of the SSD-based FlashEigen against multiple
in-memory implementations including the original Anasazi eigensolvers.
We first evaluate the performance
of the two most computationally intensive computation in the eigensolvers:
sparse matrix dense matrix multiplication and dense matrix multiplication.
They together account for most of the running time of eigendecomposition.
We demonstrate the effectiveness of each optimization on the two operations.
Then we compare the performance of our external-memory implementation with
multiple in-memory implementations: \textit{(i)} our in-memory implementations,
\textit{(ii)} MKL and \textit{(iii)} the Trilinos implementations. We then
evaluate the overall
performance of FlashEigen and compare with the original Anasazi eigensolvers
in both shared memory and shared memory. We further demonstrate the scalability
of FlashEigen on a web graph of 3.4 billion vertices and 129 billion edges.

We conduct all experiments on a non-uniform memory architecture machine with
four Intel Xeon E7-4860 processors, clocked at 2.6 GHz, and 1TB memory of
DDR3-1600. Each processor has 12 cores. The machine has three LSI SAS 9300-8e
host bus adapters (HBA) connected to a SuperMicro storage chassis, in which
24 OCZ Intrepid 3000 SSDs are installed. The 24 SSDs together deliver
approximately XXX GB/s for read and XXX GB/s for write. The machine runs
Linux kernel v3.13.0. We use 48 threads for most of experiments by default.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
Graph datasets & \# Vertices & \# Edges & Directed \\
\hline
Twitter \cite{twitter} & $42$M & $1.5$B & Yes \\
\hline
Friendster \cite{} & $65$M & $1.7$B & No \\
\hline
W \cite{} & $62$M & $12$B & No \\
\hline
Page \cite{web_graph} & $3.4$B & $129$B & Yes \\
%\hline
%RMAT graph \cite{rmat} & & & & \\
\hline
\end{tabular}
\normalsize
\end{center}
\vspace{-10pt}
\caption{Graph data sets.}
\label{graphs}
\end{table}

We use the real-world graphs in Table \ref{graphs} for evaluation. The largest
graph is the page graph with 3.4 billion vertices and 129 billion edges.
The smallest graph we use has 42 million vertices and 1.5 billion edges.
W is a similarity graph generated from XXX. The degree of the vertices in
this graph varies from X to XXX and does not follow the power-law distribution.
The page graph is clustered by domain, generating good CPU cache hit rates
in sparse matrix dense matrix multiplication.

\subsection{Sparse matrix multiplication}
This section evaluates the performance of the in-memory (IM) and
semi-external-memory (SEM) implementations of SpMM in FlashEigen. We first
evaluate the effectiveness of the optimizations on the IM-SpMM implementations.
Then we compares its performance
with that of the in-memory (IM) implementation, the Intel MKL implementation
and the Trilinos implementation.

We apply a set optimizations to the IM-SpMM implementation. To understand
their effectiveness, we start with an implementation that performs sparse
matrix multiplication on a sparse matrix in the CSR format and apply
the optimizations incrementally in the following order:
\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
	\item partition dense matrices for NUMA (NUMA),
	\item partition the sparse matrix in both dimension into tiles of
		$16K \times 16K$ for cache blocking (Cache blocking),
	\item organize multiple physical tiles into a super tile (Super tile),
	\item use CPU vectorization instructions (Vec),
	\item allocate a local buffer to store the intermediate result when
		processing tile rows (Local write),
	\item combine the SCSR and COO format to reduce the number of conditional
		jump CPU instructions (SCSR+COO),
\end{itemize}
Figure \ref{perf:spmm_opt} shows that almost all of these optimizations have
positive effect
on the sparse matrix multiplication but the degree of effectiveness varies
significantly between different graphs and different numbers of columns on
the dense matrices. For example, the NUMA optimization is more effective when
the dense matrices have more columns. Cache blocking is very effective when
the dense matrices have fewer columns and it even has a little negative effect
on the Friendster graph when the dense matrices have 16 columns. The reason is
that the tile of $16K \times 16K$ is too large for the dense matrices with
16 columns and the rows from the dense matrices can no longer be kept in
the CPU cache during the multiplication on the tile. In the KrylovSchur
eigensolver, we never use dense matrices with more than four columns in
sparse matrix multiplication.

\dz{We might want to show the effectiveness of load balancing as well.}

\begin{figure}
	\begin{center}
		\footnotesize
		\vspace{-15pt}
		\include{SpMM_optimize}
		\vspace{-15pt}
		\caption{The effectiveness of the SpMM optimizations on the Friendster
			graph (F) and the Twitter graph (T) for different numbers of
			columns in the dense matrices.}
		\label{perf:spmm_opt}
	\end{center}
\end{figure}

\dz{I should compare IM-SpMM and SEM-SpMM in more details, maybe on all
sparse matrices.}
to show that SEM SpMM will eventually catch the performance of in-mem SpMM. 

We then evaluate the performance of the IM-SpMM and SEM-SpMM in FlashEigen
and compare their performance with the MKL implementation and the Trilinos
implementation (Figure \ref{perf:spmm}). We only show the performance of
SpMM on the Friendster graph and the performance on the other graphs is very
similar. The MKL and Trilinos SpMM cannot run on the page graph.
IM-SpMM significantly outperforms both the MKL and Trilinos implementations.
In the closest case, i.e., Trilinos with the one-column dense matrix,
IM-SpMM outperforms Trilinos by 36\%. This suggests that the sparse matrix
multiplication is optimized for SpMV. The MKL SpMM can perform well on
the case that the dense matrices have more columns, but it cannot balance
the load well on a power-law graph. \dz{How about on W?}

\begin{figure}
	\begin{center}
		\footnotesize
		\vspace{-15pt}
		\include{SpMM-Friendster}
		\vspace{-15pt}
		\caption{The runtime of in-memory SpMM (FE-IM) and SEM-SpMM (FE-SEM)
			in the FlashEigen, the MKL and the Trilinos implementation on
		the Friendster graph.}
		\label{perf:spmm}
	\end{center}
\end{figure}

\subsection{Dense matrix multiplication}

We evaluate the performance of dense matrix multiplication, which is the other
most computationally intensive operation in the eigensolvers. We compare our
in-memory and external-memory matrix multiplication with the MKL and Trilinos
implementations.

In eigendecomposition, there are two forms of dense matrix multiplication.
The first form, shown by $op1$ in Table \ref{anasazi_ops}, multiplies
a tall-and-skinny dense matrix of $n \times m$
with a small dense matrix of $m \times b$ and output a tall-and-skinny dense
matrix of $n \times b$, where $n$ is the size of the eigenvalue problem,
$m$ is the number of existing vectors in the subspace and $b$ is the block
size of the eigensolver. The second form, shown by $op3$, multiplies
a transpose of a tall-and-skinny dense matrix
of $n \times m$ with another tall-and-skinny dense matrix of $n \times b$
and output a small matrix. $m$ varies from one block size to the total number
of vectors in the subspace and is increased by one block size in the next
iteration. In the experiments, we set $n$ to 60M, $b$ to 4 and vary $m$
from $4$ to $512$. This setting of $b$ and $m$ is used in the experiments
of our external-memory KrylovSchur eigensolver in the next section.

Figure \ref{perf:gemm} and \ref{perf:mvtransmv} show the performance of
the in-memory and external-memory dense matrix multiplication.
The external-memory multiplication in FlashEigen is roughly 3-5 times slower
than the in-memory implementation. In the dense matrix multiplication of
the first form, the in-memory implementation outperforms MKL when the number
of the vectors exceeds $128$ and MKL does not have a parallel implementation
for the dense matrix multiplication of the second form. \dz{Show the performance
of the Trilinos implementation.}

\dz{demonstrate that SAFS outperforms software RAID for sequential I/O access.}

\dz{Individual optimizations on dense matrix multiplication.}

\dz{demonstrate that storing vectors in multiple matrices outperforms storing
them in a single matrix.}

\begin{figure}
	\begin{center}
		\footnotesize
		\vspace{-15pt}
		\include{GEMM}
		\vspace{-15pt}
		\caption{The runtime of dense matrix multiplication in $op1$. We compare
			in-memory (FE-IM) and external-memory (FE-EM) implementations
			in FlashEigen with the MKL and Trilinos implementations.}
		\label{perf:gemm}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\footnotesize
		\vspace{-15pt}
		\include{MvTransMv}
		\vspace{-15pt}
		\caption{The runtime of dense matrix multiplication in $op3$. We compare
			in-memory (FE-IM) and external-memory (FE-EM) implementations
			in FlashEigen with the Trilinos implementations.}
		\label{perf:mvtransmv}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\footnotesize
		\vspace{-15pt}
		\include{DMM-IO}
		\vspace{-15pt}
		\caption{The I/O throughput of the dense matrix multiplication
		on the SSD array.}
		\label{perf:dmm_io}
	\end{center}
\end{figure}

The external-memory dense matrix multiplication has almost saturated
the I/O bandwidth of the SSDs (Figure \ref{perf:dmm_io}). The average
I/O throughput reaches 10.24GB/s from the entire SSD array or 437MB/s
per SSD, close to the maximal I/O throughput of the SSDs.
This also indicates that the SSDs are the bottleneck for
the dense matrix multiplication in the eigensolver. This is not
surprising because the sequential I/O performance of SSDs is an order of
magnitude smaller than RAM. 

\subsection{Compare EM eigensolvers with in-memory eigensolvers}
We compare our EM eigensolvers with our in-mem eigensolvers as well as the original
Anasazi eigensolvers. For the in-mem eigensolver, it should also have two versions.
One version is to materialize all dense matrix operations and the other version is
to materialize virtual matrices in the same approach as the EM eigensolvers.

\subsubsection{Tuning the eigensolver for faster convergence}
We need to trade off memory/disk space with speed and the number of writes.
The larger column matrix we store, the fewer iterations are required. However,
it is unclear how a large column matrix affects the amount of data written disks.
If we implement dense matrix operations naively, a large column matrix helps to
reduce the amount of data written to disks; if we avoid materializing a dense
matrix every time we generate a dense matrix, a smaller column matrix potentially
reduce the amount of data written to disks.

The block eigensolvers have two parameters that significantly affect
the convergence rate. One is the block size, which determines the number
of vectors in the subspace that operate together. The other is the number
of blocks. Table \ref{eigen_conf} summaries the configuration that performs
well in general for different numbers of eigenpairs and different eigenvalue
problems. The LOBPCG eigensolver is the special case that only allows users
to specify the block size.

The computation complexity increases roughly linearly with the number of blocks
and the square of the block size.

\subsubsection{Performance of eigensolvers}

When computing hundreds of eigenpairs, an EM eigensolver may be able to outperform
an in-mem version.

To compute hundreds of eigenpairs, we need a large block size, which means
the dense matrix has a large number of columns. EM multiplication of such
a large matrix has performance similar to in-mem version. Furthermore,
to compute such a large number of eigenpairs, we need a lot of memory to
store the vectors in the basis. The EM eigensolver can store more vectors
for the basis and thus has a faster convergance rate.

We need to show how much read and write we save with lazy evaluation. We should
also show how much multiplication the lazy evaluation causes.

\dz{report I/O to show how well the optimizations can reduce I/O.}

\dz{decompose the runtime of an eigensolver.}

\dz{Performance, I/O vs. the number of cached matrices}

\subsubsection{External memory vs. distributed memory}

\subsubsection{FlashEigen on the page graph}
