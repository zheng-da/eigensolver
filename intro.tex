% What problem are you going to solve.
Spectral analysis \cite{} is a fundamental tool for both graph analysis and
other areas of data mining. Essentially, it computes
eigenvalues and eigenvectors of graphs to infer properties of graphs.
spectral clustering \cite{Ng01, Sussman12}, triangle counting \cite{Tsourakakis08}.
Many real-world graphs are massive: Facebook's social network has billions
of vertices and today's web graph is even much larger.

% Why is it hard?

It is computationally expensive to compute all eigenvalues and
eigenvectors of a large matrix. The computation complexity is
$O(n^3)$ on a square matrix \cite{Pan99}, where $n$ is the number of rows
and columns of the matrix.
When the size of a matrix grows to millions or even billions of rows and
columns, it becomes prohibitive to compute all eigenvalues and
eigenvectors.

Numerous algorithms \cite{Lanczos, IRLM, krylovschur, Arbenz05} have been
developed to compute a small number of eigenpairs.
The current popular eigensolver packages such as ARPACK \cite{arpack}
and Anasazi \cite{anasazi} have state-of-art eigensolvers
capable of computing a few eigenvalues with certain properties such as
the eigenvalues of the largest or smallest magnitude. All of these eigensolvers
perform a sequence of sparse matrix multiplication to construct and update
a vector subspace $S \in \mathbb{R}^{n \times m}$, where $n$ is the size of
the eigenproblem and $m$ is the subspace size \cite{Arbenz05}. In addition,
they perform matrix operations on the vector subspace. When computing eigenpairs
of a graph at the billion scale, neither the sparse matrix that represents
a graph nor the vector subspace fits in the RAM of a single machine.

It is challenging to implement an efficient kernel of sparse matrix
multiplication for many real-world graphs. Sparse matrix multiplication
on these graphs induces many small random memory access due to near-random
vertex connection. It may suffer from load imbalancing because of
the power-law distribution in vertex degree. Furthermore, graphs cannot be
clustered or partitioned effectively \cite{leskovec} to localize access.

%How have others addressed the problem?
Large-scale eigendecomposition is generally solved in a large cluster
\cite{anasazi, slepc}, where the aggregate memory is sufficient to store
the sparse matrix and the vector subspace. Sparse matrix multiplication
on graphs in distributed memory leads to significant network communication
and is usually bottlenecked by the network. As such, this operation requires
fast network to achieve performance. However, a supercomputer or a large
cluster with fast network communication is not accessible to many people.

%What is the nature of your solution?

We build FlashEigen, an external-memory eigensolver, on top of a user-space
filesystem called SAFS \cite{safs} to solve a large eigenproblem with SSDs
in a single machine. 
%Today's SSDs or SSD arrays
%are capable of delivering over a million random IOPS or multiple GB/s of
%sequential I/O \cite{safs}. Given such I/O performance, it is possible to
%achieve nearly in-memory performance for large-scale graph analysis using SSDs
%\cite{flashgraph}. In this paper, we focus on utilizing SSDs for
%spectral graph analysis as well as general linear algebra.
Instead of developing a new eigensolver from scratch, we leverage
the Anasazi framework and implement SSD-based matrix operations
for the framework. FlashEigen is specifically optimized for the Block
Krylov-Schur \cite{krylovschur} eigensolver because it is the fastest and
generates the least I/O among the Anasazi eigensolvers when computing
eigenvalues of many power-law graphs. The Anasazi eigensolvers implement
a block extension, which enables the eigensolvers to update multiple vectors
in the subspace in each iteration to increase computation density and amortize
I/O overhead. As a result, the eigensolvers require sparse matrix dense matrix
multiplication (SpMM). Given a graph with hundreds of millions of vertices or
even billions of vertices,
the vector subspace constructed by the eigensolvers requires very large storage
size, usually much larger than the sparse matrix. Therefore, FlashEigen stores
the entire vector subspace on SSDs.

% Why is it new/different/special?

Although SSDs can deliver high IOPS and sequential I/O throughput, we have
to overcome many technical challenges to construct an external-memory
eigensolver with performance comparable to in-memory eigensolvers.
First, it is challenging to achieve the maximal I/O throughput, on the order
of ten gigabytes, from a large array of commodity SSDs, due to many overheads
from the operating system. Even achieving the maximal I/O throughput, SSDs are
still an order of magnitude slower than DRAM in throughput. Furthermore,
SSDs wear out after we write data to them. For example, some enterprise
SSDs \cite{ocz} only allows one DWPD (diskful writes per day). Writing too much
data to these SSDs drastically shortens their lives and increases operation
cost.

We perform SpMM in a semi-external memory (SEM) fashion, which keeps
the sparse matrix on SSDs and dense matrices in memory. This operation streams
rows in the sparse matrix to memory and multiplies with the dense matrix in
memory, which generates sequential I/O and allows us to yield maximal I/O
throughput from SSDs. While maximizing the I/O throughput, we also compress
the sparse matrix to further accelerate retrieving the sparse matrix from SSDs.
The SEM strategy incorporates well with in-memory optimizations for SpMM.
We deploy multiple in-memory optimizations specifically designed for power-law
graphs. For example, we assign partitions of the sparse matrix dynamically to
threads for load balancing, deploy cache blocking to increase CPU cache hits,
partition and evenly distribute the dense matrix to NUMA nodes to fully utilize
the memory bandwidth of a NUMA machine.

We deploy optimizations on dense matrix operations to reduce I/O and fully
utilize the I/O bandwidth of the SSDs. Thanks to the block extension of
the Anasazi eigensolvers, we group multiple vectors together into a column-major
dense matrix and store each dense matrix in a separate SAFS file for efficient
I/O access to any vectors in the subspace. When accessing a large number of dense
matrices in a single operation, we group dense matrices to constrain memory
consumption of the matrix operation to improve its scalability. To reduce I/O
and alleviate SSD wear out, we use deploy lazy evaluation and cache the most
recent dense matrix in the subspace.

% What are it's key features?

Our result shows that for many real-world sparse graphs, the SSD-based
eigensolver is able to achieve 40\%-60\%
performance of its in-memory implementation and has performance comparable to
the Anasazi eigensolver on a machine with 48 CPU cores for computing various
numbers of eigenvalues. We further demonstrate that the SSD-based eigensolver
is capable of scaling to a graph with 3.4 billion vertices and 129 billion edges.
It takes about 4 hours to compute eight eigenvalues of the billion-node graph
and use 120 GB memory. We conclude that our solution offers new design
possibilities for large-scale eigendecomposition, replacing memory with larger
and cheaper SSDs and processing bigger problems on fewer machines.
